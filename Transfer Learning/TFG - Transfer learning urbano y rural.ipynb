{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76bd2a8",
   "metadata": {},
   "source": [
    "## Importaciones de las librerías y paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c26a0",
   "metadata": {},
   "source": [
    "## Creación de la clase heredada de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = \"G:/dataset_rural_y_urbano/train\"\n",
    "TRAIN_MASKS_PATH = \"G:/dataset_rural_y_urbano/train_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b93c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentation(Dataset):\n",
    "    def __init__(self,img_dir, mask_dir, transform = None):\n",
    "        self.transforms = transform\n",
    "        self.images = sorted(os.listdir(img_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img = np.array(Image.open(TRAIN_IMAGES_PATH + \"/\" + self.images[index]))\n",
    "        mask = np.array(Image.open(TRAIN_MASKS_PATH + \"/\" + self.masks[index]))\n",
    "        if self.transforms is not None:\n",
    "            transforms = self.transforms(image=img,mask=mask)\n",
    "            img = transforms['image']\n",
    "            mask = transforms['mask']\n",
    "            mask = torch.max(mask,dim=2)[0]\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb05990",
   "metadata": {},
   "source": [
    "## Obtención de los conjuntos de entrenamiento, validación y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919161fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(image_dir, mask_dir, transform = None, batch_size=1, shuffle=True, pin_memory=True):\n",
    "    \n",
    "    semantic_segmentation = SemanticSegmentation(image_dir, mask_dir, transform = transform)\n",
    "    \n",
    "    train_size = int(0.8 * len(semantic_segmentation))\n",
    "    val_size = int(0.1 *len(semantic_segmentation))\n",
    "    test_size = len(semantic_segmentation) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(semantic_segmentation, [train_size, val_size, test_size])\n",
    "    \n",
    "    train_set = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "    val_set = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "    test_set = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "                        A.Resize(256,480),\n",
    "                        ToTensorV2()\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = get_images(TRAIN_IMAGES_PATH, TRAIN_MASKS_PATH, transform=transforms, batch_size=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db0bb0",
   "metadata": {},
   "source": [
    "## Para almacenar los distintos conjuntos de entrenamiento, validación y testeo (si ya se tienen los conjuntos, se salta este paso y se cargan en el siguiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the training set\n",
    "with open('G:/dataset_rural_y_urbano/saved_sets/train_set_dataset_rural_y_urbano.pkl', 'wb') as f:\n",
    "    pickle.dump(train_set, f)\n",
    "\n",
    "# To save the validation set\n",
    "with open('G:/dataset_rural_y_urbano/saved_sets/val_set_dataset_rural_y_urbano.pkl', 'wb') as f:\n",
    "    pickle.dump(val_set, f)\n",
    "    \n",
    "# To save the test set\n",
    "with open('G:/dataset_rural_y_urbano/saved_sets/test_set_dataset_rural_y_urbano.pkl', 'wb') as f:\n",
    "    pickle.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24fe23",
   "metadata": {},
   "source": [
    "## Se pueden cargar los conjuntos si ya se tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G:/dataset_rural_y_urbano/saved_sets/train_set_dataset_rural_y_urbano.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "    \n",
    "with open('G:/dataset_rural_y_urbano/saved_sets/train_set_dataset_rural_y_urbano.pkl', 'rb') as f:\n",
    "    val_set = pickle.load(f)\n",
    "    \n",
    "with open('G:/dataset_rural_y_urbano/saved_sets/train_set_dataset_rural_y_urbano.pkl', 'rb') as f:\n",
    "    test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ed5f3",
   "metadata": {},
   "source": [
    "## Elección del dispositivo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bf311",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4daa87",
   "metadata": {},
   "source": [
    "   ### Opción 1) Se carga el modelo entrenado con el dataset rural y se se modifica la última clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96358e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('G:/dataset_rural/saved_models/modelo_guardado_dataset_rural_DeepLabV3_Epoch_23.pt')\n",
    "model = model.to('cuda')\n",
    "\n",
    "num_new_classes = 14\n",
    "\n",
    "model.classifier[-1] = nn.Conv2d(256, num_new_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "model.aux_classifier[-1] = nn.Conv2d(256, num_new_classes, kernel_size=(1, 1), stride=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156aa69",
   "metadata": {},
   "source": [
    "   ### Opción 2) Se carga el modelo entrenado con el dataset rural pero con las clases que no están en el dataset urbano puestas como clase clutter, y se se modifica la última clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('G:/dataset_rural/saved_models/modelo_guardado_dataset_rural_con_clutter_DeepLabV3_Epoch_20.pt')\n",
    "model = model.to('cuda')\n",
    "\n",
    "num_new_classes = 9\n",
    "\n",
    "model.classifier[-1] = nn.Conv2d(256, num_new_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "model.aux_classifier[-1] = nn.Conv2d(256, num_new_classes, kernel_size=(1, 1), stride=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8288268",
   "metadata": {},
   "source": [
    "## Definiendo hiperparámetros y opciones del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e750f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e318b1",
   "metadata": {},
   "source": [
    "## Función que devuelve a partir de una predicción la matriz con el tamaño original de la máscara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def originalMatrix(arr):\n",
    "    if isinstance(arr, torch.Tensor):\n",
    "        arr = arr.cpu().numpy()\n",
    "        \n",
    "    # Convert the original matrix to a matrix with zeros in the columns of G and B\n",
    "    converted_matrix = np.zeros(arr.shape + (3,))\n",
    "    \n",
    "    # Copy the values of R into the column of R\n",
    "    converted_matrix[..., 0] = arr  \n",
    "    \n",
    "    return converted_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7381cb",
   "metadata": {},
   "source": [
    "## Función que pasa la máscara a los colores originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acbffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def originalColorMask(converted_matrix):\n",
    "    \n",
    "    # New colors\n",
    "    new_colors = np.array([\n",
    "        [1, 0, 0],\n",
    "        [2, 0, 0],\n",
    "        [3, 0, 0],\n",
    "        [4, 0, 0],\n",
    "        [5, 0, 0],\n",
    "        [6, 0, 0],\n",
    "        [7, 0, 0],\n",
    "        [8, 0, 0],\n",
    "        [9, 0, 0],\n",
    "        [10, 0, 0],\n",
    "        [11, 0, 0],\n",
    "        [12, 0, 0],\n",
    "        [13, 0, 0]\n",
    "    ])\n",
    "\n",
    "    # Originals colors\n",
    "    original_colors = np.array([\n",
    "        [127, 127, 0],\n",
    "        [0, 255, 0],\n",
    "        [0, 127, 0],\n",
    "        [255, 255, 0],\n",
    "        [255, 127, 0],\n",
    "        [255, 255, 255],\n",
    "        [255, 0, 255],\n",
    "        [127, 127, 127],\n",
    "        [0, 0, 255],\n",
    "        [0, 255, 255],\n",
    "        [127, 127, 63],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 0]\n",
    "    ])\n",
    "\n",
    "    original_arr = converted_matrix.astype(int)\n",
    "\n",
    "    # Crea una nueva matriz NumPy con los nuevos colores\n",
    "    new_colors_arr = np.zeros_like(original_arr)\n",
    "    for i in range(len(original_colors)):\n",
    "        mask = np.all(original_arr == new_colors[i], axis=-1)\n",
    "        new_colors_arr[mask] = original_colors[i]\n",
    "\n",
    "    # Convierte la matriz NumPy de nuevo a una imagen y guarda en el directorio de salida\n",
    "    new_image = Image.fromarray(new_colors_arr.astype(np.uint8))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f58e6",
   "metadata": {},
   "source": [
    "## Función que pasa la predicción a los colores originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa72896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def originalColorPrediction(converted_matrix):\n",
    "    \n",
    "    # Originals colors\n",
    "    original_colors = np.array([\n",
    "        [127, 127, 0],\n",
    "        [0, 255, 0],\n",
    "        [0, 127, 0],\n",
    "        [255, 255, 0],\n",
    "        [255, 127, 0],\n",
    "        [255, 255, 255],\n",
    "        [255, 0, 255],\n",
    "        [127, 127, 127],\n",
    "        [0, 0, 255],\n",
    "        [0, 255, 255],\n",
    "        [127, 127, 63],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    # New colors\n",
    "    new_colors = np.array([\n",
    "        [1, 0, 0],\n",
    "        [2, 0, 0],\n",
    "        [3, 0, 0],\n",
    "        [4, 0, 0],\n",
    "        [5, 0, 0],\n",
    "        [6, 0, 0],\n",
    "        [7, 0, 0],\n",
    "        [8, 0, 0],\n",
    "        [9, 0, 0],\n",
    "        [10, 0, 0],\n",
    "        [11, 0, 0],\n",
    "        [12, 0, 0],\n",
    "        [13, 0, 0]\n",
    "    ])\n",
    "\n",
    "\n",
    "    original_arr = converted_matrix.astype(int)[0]\n",
    "\n",
    "    # Create a new NumPy array with the new colors\n",
    "    new_colors_arr = np.zeros_like(original_arr)\n",
    "    for i in range(len(original_colors)):\n",
    "        mask = np.all(original_arr == new_colors[i], axis=-1)\n",
    "        new_colors_arr[mask] = original_colors[i]\n",
    "\n",
    "    # Convert the NumPy array back to an image\n",
    "    new_image = Image.fromarray(new_colors_arr.astype(np.uint8))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa923f77",
   "metadata": {},
   "source": [
    "## Se definen las funciones que calcula IOU y DICE durante el entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou_training(img1, img2):\n",
    "    \n",
    "    img1 = np.asarray(img1)\n",
    "    img2 = np.asarray(img2)\n",
    "    \n",
    "    # We calculate the intersection between the two pixel matrices\n",
    "    intersection = np.all(img1 == img2, axis=-1)\n",
    "\n",
    "    # We count the total pixels that intersect\n",
    "    total_intersection = np.count_nonzero(intersection)\n",
    "    \n",
    "    # We count the total number of pixels in both images\n",
    "    total_pixels = img1.shape[0] * img1.shape[1] + img2.shape[0] * img2.shape[1]\n",
    "    \n",
    "    # We calculate the union\n",
    "    union = total_pixels - total_intersection\n",
    "    \n",
    "    return total_intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_training(im1, im2):\n",
    "        \n",
    "    img1 = np.asarray(im1)\n",
    "    img2 = np.asarray(im2)\n",
    "    \n",
    "    # We calculate the intersection between the two pixel matrices\n",
    "    intersection = np.all(img1 == img2, axis=-1)\n",
    "\n",
    "    # We count the total pixels that intersect\n",
    "    total_intersection = np.count_nonzero(intersection)\n",
    "    \n",
    "    width, height = im1.size\n",
    "    num_pixeles_im1 = width * height\n",
    "    \n",
    "    width, height = im2.size\n",
    "    num_pixeles_im2 = width * height\n",
    "    \n",
    "    # Calculate the DICE coefficient\n",
    "    dice = (2. * total_intersection) / (num_pixeles_im1 + num_pixeles_im2)\n",
    "    \n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a5226",
   "metadata": {},
   "source": [
    "## Función que devuelve el valor de IOU y DICE en función de TP, FP  y FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(colorMaskOriginal, colorPredsOriginal):\n",
    "\n",
    "    count = {}\n",
    "\n",
    "    equivalence_colors = {\n",
    "        (127, 127, 63):\"Hill\", \n",
    "        (0, 127, 0):\"Forest\",\n",
    "        (255, 255, 0):\"Residential\",\n",
    "        (0, 255, 0):\"Land\",\n",
    "        (255, 0, 255):\"Church\",\n",
    "        (255, 255, 255):\"Road\",\n",
    "        (127, 127, 0):\"Fence\",\n",
    "        (255, 0, 0):\"Person\",\n",
    "        (127, 127, 127):\"Car\",\n",
    "        (0, 255, 255):\"Sky\",\n",
    "        (255, 127, 0):\"Haystack\",\n",
    "        (0, 0, 255):\"Water\",\n",
    "        (0, 0, 0):\"Clutter\",\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "    for rgb, name in equivalence_colors.items():\n",
    "        count[rgb] = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0}\n",
    "\n",
    "\n",
    "    for a in range(colorMaskOriginal.width):\n",
    "        for b in range(colorMaskOriginal.height):\n",
    "            for category, color in equivalence_colors.items():\n",
    "                if colorPredsOriginal.getpixel((a,b)) == category and colorMaskOriginal.getpixel((a,b)) == category:\n",
    "                    count[category]['TP'] += 1\n",
    "                elif colorPredsOriginal.getpixel((a,b)) != category and colorMaskOriginal.getpixel((a,b)) != category:\n",
    "                    count[category]['TN'] += 1\n",
    "                elif colorPredsOriginal.getpixel((a,b)) == category and colorMaskOriginal.getpixel((a,b)) != category:\n",
    "                    count[category]['FP'] += 1\n",
    "                elif colorPredsOriginal.getpixel((a,b)) != category and colorMaskOriginal.getpixel((a,b)) == category:\n",
    "                    count[category]['FN'] += 1   \n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0 \n",
    "    for category, data in count.items():\n",
    "        tp += data[\"TP\"]\n",
    "        fp += data[\"FP\"]\n",
    "        tn += data[\"TN\"]\n",
    "        fn += data[\"FN\"]\n",
    "      \n",
    "    valueIOU = tp / (fp + tp + fn)\n",
    "    valueDICE = (2 * tp) / (fp + (2 * tp) + fn)\n",
    "        \n",
    "    return valueIOU, valueDICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c63f6",
   "metadata": {},
   "source": [
    "## Función de validación para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3500e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    cost = 0.    \n",
    "    dice = 0.\n",
    "    iou = 0.\n",
    "    images_count = 0\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            \n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            y = y.type(torch.long)\n",
    "                        \n",
    "            x = x.float()\n",
    "            preds = model(x)\n",
    "            \n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            predictions = torch.argmax(softmax(model(x.float())['out']), axis=1).to(DEVICE)\n",
    "            \n",
    "            cost += loss_fn(preds, y)            \n",
    "        \n",
    "            correct += (predictions == y).sum().item()\n",
    "            total += y.numel()\n",
    "            \n",
    "            for i in range(x.shape[0]): \n",
    "                \n",
    "                prediction = predictions[i]\n",
    "                prediction = prediction.unsqueeze(0)\n",
    "\n",
    "                mask = y[i].squeeze().cpu().numpy()\n",
    "\n",
    "                matrizPredsOriginal = originalMatrix(prediction)\n",
    "                colorPredsOriginal = originalColorPrediction(matrizPredsOriginal)\n",
    "\n",
    "                matrizMaskOriginal = originalMatrix(mask)\n",
    "                colorMaskOriginal = originalColorMask(matrizMaskOriginal)\n",
    "                \n",
    "                dice += dice_coef_training(colorPredsOriginal, colorMaskOriginal)         \n",
    "                iou += calculate_iou_training(colorPredsOriginal, colorMaskOriginal)\n",
    "                \n",
    "                images_count += 1\n",
    "     \n",
    "                \n",
    "        return cost/len(loader), correct/total, dice/images_count, iou/images_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065746a",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e46fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_every = 33\n",
    "early_stop = False\n",
    "best_valid_loss = np.Inf\n",
    "epochs_no_improve = 0\n",
    "max_epochs_stop = 3\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "validation_costs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    mb = 0\n",
    "    train_cost_acum = 0.\n",
    "    dice_train = 0.\n",
    "    iou_train = 0.\n",
    "    count_images = 0\n",
    "\n",
    "    loop = tqdm(enumerate(train_set),total=len(train_set))\n",
    "    for batch_idx, (data, targets) in loop:\n",
    "        data = data.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "        targets = targets.to(DEVICE)\n",
    "        targets = targets.type(torch.long)\n",
    "        \n",
    "        # Forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            data = data.half()                           # Esta línea la he puesto yo\n",
    "            predictions = model(data)['out']\n",
    "            loss = loss_fn(predictions, targets) \n",
    "            \n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            predictions = torch.argmax(softmax(predictions),axis=1).to(DEVICE)\n",
    "            \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        mb += 1\n",
    "        train_cost_acum += loss.item()\n",
    "\n",
    "        # Calculate IOU and DICE values from the current training batch\n",
    "        for i in range(data.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                prediction = predictions[i]\n",
    "                prediction = prediction.unsqueeze(0)\n",
    "\n",
    "\n",
    "            mask = targets[i].squeeze().cpu().numpy()     \n",
    "\n",
    "            # Both the mask and the prediction are obtained with the original colors\n",
    "            matrizPredsOriginal = originalMatrix(prediction)\n",
    "            colorPredsOriginal = originalColorPrediction(matrizPredsOriginal)\n",
    "\n",
    "            matrizMaskOriginal = originalMatrix(mask)\n",
    "            colorMaskOriginal = originalColorMask(matrizMaskOriginal)\n",
    "\n",
    "            dice_train += dice_coef_training(colorPredsOriginal, colorMaskOriginal)         \n",
    "            iou_train += calculate_iou_training(colorPredsOriginal, colorMaskOriginal)\n",
    "\n",
    "            count_images += 1\n",
    "\n",
    "        # The results are displayed and the model is stored\n",
    "        if mb%store_every == 0:\n",
    "            val_cost, val_acc, dice, iou = validation(model, val_set)\n",
    "            train_cost_every = float(train_cost_acum)/mb\n",
    "            \n",
    "            validation_costs.append(val_cost)\n",
    "\n",
    "            print(f'epoch: {epoch}, train cost: {train_cost_every:.4f}, val cost: {val_cost:.4f}, val acc: {val_acc:.4f},'\n",
    "                    f'train dice: {dice_train/count_images}, train iou: {iou_train/count_images}, val dice: {dice}, val iou: {iou}')\n",
    "\n",
    "            file_name = \"G:/transfer_learning/saved_models/modelo_guardado_transfer_learning_Epoch_\" + str(epoch) + \".pt\"\n",
    "            torch.save(model, file_name)\n",
    "            \n",
    "            if val_cost < best_valid_loss:\n",
    "                best_valid_loss = val_cost\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "                # If the validation cost does not improve by max_epochs_stop, stop the training\n",
    "                if epochs_no_improve == max_epochs_stop:\n",
    "                    early_stop = True\n",
    "                    print(\"Early stopping!\")\n",
    "                    print(\"La época con el coste de validación más bajo es la época: \" + str(epoch - 3))\n",
    "                    break\n",
    "                    \n",
    "        if epochs_no_improve == max_epochs_stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6d701",
   "metadata": {},
   "source": [
    "## Para cargar el mejor modelo una vez que se haya concluido el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('G:/transfer_learning/saved_models/modelo_guardado_transfer_learning_Epoch_13.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5867e21",
   "metadata": {},
   "source": [
    "## Obtención de los TP, TN, FP y FN para el conjunto de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_values = {}\n",
    "model = model.to('cpu')\n",
    "\n",
    "color_equivalence = {\n",
    "        '(127, 127, 63)':\"Hill\", \n",
    "        '(0, 127, 0)':\"Forest\",\n",
    "        '(255, 255, 0)':\"Residential\",\n",
    "        '(0, 255, 0)':\"Land\",\n",
    "        '(255, 0, 255)':\"Church\",\n",
    "        '(255, 255, 255)':\"Road\",\n",
    "        '(127, 127, 0)':\"Fence\",\n",
    "        '(255, 0, 0)':\"Person\",\n",
    "        '(127, 127, 127)':\"Car\",\n",
    "        '(0, 255, 255)':\"Sky\",\n",
    "        '(255, 127, 0)':\"Haystack\",\n",
    "        '(0, 0, 255)':\"Water\",\n",
    "        '(0, 0, 0)':\"Clutter\",\n",
    "}\n",
    "\n",
    "\n",
    "for rgb, name in color_equivalence.items():\n",
    "    container_values[rgb] = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0, 'IOU':0, 'total':0, 'coincidentes':0}\n",
    "\n",
    "    \n",
    "for x1, y in test_set:\n",
    "    \n",
    " \n",
    "    # The TP, TN, FP and FN of the first 4 images of the current batch are obtained\n",
    "    x = x1.float().to('cpu')[0:4]\n",
    "    predictions = model(x)\n",
    "    targets = y[0:4]\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    preds = torch.argmax(softmax(predictions['out']), axis=1).to('cpu')\n",
    "\n",
    "    for i in range(x.shape[0]):  \n",
    "        with torch.no_grad():\n",
    "            prediction = preds[i].unsqueeze(0)\n",
    "            mask = targets[i].squeeze().cpu().numpy()\n",
    "\n",
    "            matrizPredsOriginal = originalMatrix(prediction)\n",
    "            colorPredsOriginal = originalColorPrediction(matrizPredsOriginal)\n",
    "\n",
    "            matrizMaskOriginal = originalMatrix(mask)\n",
    "            colorMaskOriginal = originalColorMask(matrizMaskOriginal)\n",
    "\n",
    "            for i in range(colorMaskOriginal.width):\n",
    "                for j in range(colorMaskOriginal.height):\n",
    "                    mask_color = str(colorMaskOriginal.getpixel((i,j)))\n",
    "                    container_values[mask_color]['total'] += 1\n",
    "                    for category, color in color_equivalence.items():\n",
    "                        if str(colorPredsOriginal.getpixel((i,j))) == str(category) and str(colorMaskOriginal.getpixel((i,j))) == str(category):\n",
    "                            container_values[category]['TP'] += 1\n",
    "                            container_values[category]['coincidentes'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) != str(category) and str(colorMaskOriginal.getpixel((i,j))) != str(category):\n",
    "                            container_values[category]['TN'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) == str(category) and str(colorMaskOriginal.getpixel((i,j))) != str(category):\n",
    "                            container_values[category]['FP'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) != str(category) and str(colorMaskOriginal.getpixel((i,j))) == str(category):\n",
    "                            container_values[category]['FN'] += 1\n",
    "                            \n",
    "                            \n",
    "    # The TP, TN, FP and FN of the next 4 images of the current batch are obtained                       \n",
    "    x = x1.float().to('cpu')[4:8]\n",
    "    predictions = model(x)\n",
    "\n",
    "    targets = y[4:8]\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    preds = torch.argmax(softmax(predictions), axis=1).to('cpu')\n",
    "\n",
    "    for i in range(x.shape[0]):  \n",
    "        with torch.no_grad():\n",
    "            prediction = preds[i].unsqueeze(0)\n",
    "            mask = targets[i].squeeze().cpu().numpy()\n",
    "\n",
    "            matrizPredsOriginal = originalMatrix(prediction)\n",
    "            colorPredsOriginal = originalColorPrediction(matrizPredsOriginal)\n",
    "\n",
    "            matrizMaskOriginal = originalMatrix(mask)\n",
    "            colorMaskOriginal = originalColorMask(matrizMaskOriginal)\n",
    "\n",
    "            for i in range(colorMaskOriginal.width):\n",
    "                for j in range(colorMaskOriginal.height):\n",
    "                    mask_color = str(colorMaskOriginal.getpixel((i,j)))\n",
    "                    container_values[mask_color]['total'] += 1\n",
    "                    for category, color in color_equivalence.items():\n",
    "                        if str(colorPredsOriginal.getpixel((i,j))) == str(category) and str(colorMaskOriginal.getpixel((i,j))) == str(category):\n",
    "                            container_values[category]['TP'] += 1\n",
    "                            container_values[category]['coincidentes'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) != str(category) and str(colorMaskOriginal.getpixel((i,j))) != str(category):\n",
    "                            container_values[category]['TN'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) == str(category) and str(colorMaskOriginal.getpixel((i,j))) != str(category):\n",
    "                            container_values[category]['FP'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) != str(category) and str(colorMaskOriginal.getpixel((i,j))) == str(category):\n",
    "                            container_values[category]['FN'] += 1\n",
    "                 \n",
    "    \n",
    "    # The TP, TN, FP and FN of the last 4 images of the current batch are obtained\n",
    "    x = x1.float().to('cpu')[8:13]\n",
    "    predictions = model(x)\n",
    "\n",
    "    targets = y[8:13]\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    preds = torch.argmax(softmax(predictions), axis=1).to('cpu')\n",
    "\n",
    "    for i in range(x.shape[0]):  \n",
    "        with torch.no_grad():\n",
    "            prediction = preds[i].unsqueeze(0)\n",
    "            mask = targets[i].squeeze().cpu().numpy()\n",
    "\n",
    "            matrizPredsOriginal = originalMatrix(prediction)\n",
    "            colorPredsOriginal = originalColorPrediction(matrizPredsOriginal)\n",
    "\n",
    "            matrizMaskOriginal = originalMatrix(mask)\n",
    "            colorMaskOriginal = originalColorMask(matrizMaskOriginal)\n",
    "\n",
    "            for i in range(colorMaskOriginal.width):\n",
    "                for j in range(colorMaskOriginal.height):\n",
    "                    mask_color = str(colorMaskOriginal.getpixel((i,j)))\n",
    "                    container_values[mask_color]['total'] += 1\n",
    "                    for category, color in color_equivalence.items():\n",
    "                        if str(colorPredsOriginal.getpixel((i,j))) == str(category) and str(colorMaskOriginal.getpixel((i,j))) == str(category):\n",
    "                            container_values[category]['TP'] += 1\n",
    "                            container_values[category]['coincidentes'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) != str(category) and str(colorMaskOriginal.getpixel((i,j))) != str(category):\n",
    "                            container_values[category]['TN'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) == str(category) and str(colorMaskOriginal.getpixel((i,j))) != str(category):\n",
    "                            container_values[category]['FP'] += 1\n",
    "                        elif str(colorPredsOriginal.getpixel((i,j))) != str(category) and str(colorMaskOriginal.getpixel((i,j))) == str(category):\n",
    "                            container_values[category]['FN'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74623db7",
   "metadata": {},
   "source": [
    "## Guardado del diccionario con los valores de TP, TN FP y FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G:/transfer_learning/saved_values/test_transfer_learning.csv', 'w') as archive:\n",
    "    writer = csv.writer(archive)\n",
    "    for category, value in container_values.items():\n",
    "        writer.writerow([category, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03963821",
   "metadata": {},
   "source": [
    "## Obtención de las métricas a partir de los TP, TN, FP y FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_global = 0\n",
    "fp_global = 0\n",
    "tn_global = 0\n",
    "fn_global = 0\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Clase\", \"Nombre Clase\", \"Precisión\", \"Recall\", \"Accuracy\", \"F1\", \"IOU\"]\n",
    "\n",
    "for category, data in container_values.items():\n",
    "    tp = data[\"TP\"]\n",
    "    fp = data[\"FP\"]\n",
    "    tn = data[\"TN\"]\n",
    "    fn = data[\"FN\"]\n",
    "    \n",
    "    tp_global += data[\"TP\"]\n",
    "    fp_global += data[\"FP\"]\n",
    "    tn_global += data[\"TN\"]\n",
    "    fn_global += data[\"FN\"]\n",
    "\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    f1 = 2*((precision*recall)/(precision+recall))\n",
    "    IOU_value = tp / (fp + tp + fn)\n",
    "    \n",
    "    \n",
    "    class_name = color_equivalence[category]\n",
    "    container_values[category]['IOU'] = IOU_value\n",
    "    \n",
    "    \n",
    "    table.add_row([category, class_name, f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{accuracy:.4f}\", f\"{f1:.4f}\", f\"{IOU_value:.4f}\"])\n",
    "\n",
    "\n",
    "precision_global = tp_global / (tp_global + fp_global)\n",
    "recall_global = tp_global / (tp_global + fn_global)\n",
    "accuracy_global = (tp_global + tn_global) / (tp_global + tn_global + fp_global + fn_global)\n",
    "f1_global = 2*((precision_global*recall_global)/(precision_global+recall_global))\n",
    "iou_global = tp_global / (fp_global + tp_global + fn_global)\n",
    "\n",
    "table.add_row([\"\", \"Global\", f\"{precision_global:.4f}\", f\"{recall_global:.4f}\", f\"{accuracy_global:.4f}\", f\"{f1_global:.4f}\", f\"{iou_global:.4f}\"])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad6495",
   "metadata": {},
   "source": [
    "## Obtención de una predicción de las imágenes de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_equivalence = {\n",
    "        '(127, 127, 63)':\"Hill\", \n",
    "        '(0, 127, 0)':\"Forest\",\n",
    "        '(255, 255, 0)':\"Residential\",\n",
    "        '(0, 255, 0)':\"Land\",\n",
    "        '(255, 0, 255)':\"Church\",\n",
    "        '(255, 255, 255)':\"Road\",\n",
    "        '(127, 127, 0)':\"Fence\",\n",
    "        '(255, 0, 0)':\"Person\",\n",
    "        '(127, 127, 127)':\"Car\",\n",
    "        '(0, 255, 255)':\"Sky\",\n",
    "        '(255, 127, 0)':\"Haystack\",\n",
    "        '(0, 0, 255)':\"Water\",\n",
    "        '(0, 0, 0)':\"Clutter\",\n",
    "}\n",
    "\n",
    "\n",
    "model = model.to('cpu')\n",
    "for x,y in test_set:\n",
    "    x = x.to('cpu')\n",
    "    fig , ax =  plt.subplots(1, 3, figsize=(18, 18))\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    preds = torch.argmax(softmax(model(x.float())['out']), axis=1).to('cpu')\n",
    "    img1 = np.transpose(np.array(x[0,:,:,:].to('cpu')),(1,2,0))\n",
    "    preds1 = np.array(preds[0,:,:])\n",
    "    mask1 = np.array(y[0,:,:])\n",
    "    \n",
    "    \n",
    "    ax[0].set_title('Image')\n",
    "    ax[1].set_title('Prediction')\n",
    "    ax[2].set_title('Mask')\n",
    "    \n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    # Loop through all images in the current batch\n",
    "    for i in range(x.shape[0]):\n",
    "        \n",
    "        # Get the prediction for the current image\n",
    "        prediction = preds[i]\n",
    "        prediction = prediction.unsqueeze(0)\n",
    "\n",
    "        mask = y[i].squeeze().cpu().numpy()\n",
    "        \n",
    "        matrizPredsOriginal = originalMatrix(prediction)\n",
    "        colorPredsOriginal = originalColorPrediction(matrizPredsOriginal)\n",
    "\n",
    "        matrizMaskOriginal = originalMatrix(mask)\n",
    "        colorMaskOriginal = originalColorMask(matrizMaskOriginal)\n",
    "        \n",
    "        break\n",
    "    \n",
    "    ax[0].imshow(img1)\n",
    "    ax[1].imshow(colorPredsOriginal)\n",
    "    ax[2].imshow(colorMaskOriginal)  \n",
    "    \n",
    "    break\n",
    "\n",
    "\n",
    "# Create the dictionary to store the values\n",
    "container_values = {}\n",
    "\n",
    "# Loop through the pixels of the actual image and count the pixels of each color\n",
    "for i in range(colorMaskOriginal.width):\n",
    "    for j in range(colorMaskOriginal.height):\n",
    "        color = colorMaskOriginal.getpixel((i, j))\n",
    "        if color in container_values:\n",
    "            container_values[color][\"total\"] += 1\n",
    "        else:\n",
    "            container_values[color] = {\"total\": 1, \"coincidentes\": 0}\n",
    "\n",
    "# Loop through the pixels of both images and compare their RGB value\n",
    "for i in range(colorMaskOriginal.width):\n",
    "    for j in range(colorMaskOriginal.height):\n",
    "        real_color = colorMaskOriginal.getpixel((i, j))\n",
    "        preds_color = colorPredsOriginal.getpixel((i, j))\n",
    "        if real_color == preds_color:\n",
    "            container_values[real_color][\"coincidentes\"] += 1\n",
    "\n",
    "# Print the counters for each color\n",
    "for color, data in container_values.items():\n",
    "    matching = data[\"coincidentes\"]\n",
    "    total = data[\"total\"]\n",
    "    print(f\"La clase {color_equivalence[str(color)]} tiene {matching} píxeles coincidentes de un total de {total} píxeles, es decir, un {round((matching / total)*100, 2)}%.\")\n",
    "    \n",
    "    \n",
    "iou, dice = calculate_metrics(colorPredsOriginal, colorMaskOriginal)\n",
    "print(\"\\n\\n\\nEl valor de IOU es de: \" + str(iou))\n",
    "print(\"\\nEl valor de DICE es de: \" + str(dice))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
